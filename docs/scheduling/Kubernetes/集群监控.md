## 12.3 集群进阶

### 12.3.1 资源管理

#### 1. 资源限制

在Pod或者RC定义文件中设定`resources`属性即可限制某个容器的资源使用。例如下面定义配置中：

```yaml
  template:
    metadata:
      labels:
        name: redis-master
    spec:
      cpmtaomers:
      - name: master
        image: redis
        ports:
        - containerPort: 6379
        resources:
          limits:
            cpu: 0.5
            memory: 128Mi
```

限制CPU使用为0.5（含义与前面解释Docker资源限制一样），内存使用为128Mi（十进制的内存单位）。

#### 2. 配额管理

##### **全局配额**

LimitRange资源对象可以限制特定namespace下的所有对象使用资源，以下面定义文件为例：

```yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: frontend-limit
spec:
  limits:
    - type: "pod"
      max:
        cpu: "2"
        memory: 1Gi
      min:
        cpu: "0.5"
        memory: 50Mi
    - type: "Container"
      max:
        cpu: "2"
        memory: 1Gi
      min:
        cpu: "0.25"
        memory: 20Mi
      default:
        cpu: 250m
        memory: 100Mi
```

上面配置中，限制了Pod使用CPU的大小为1-2，内存使用大小为50Mi到1Gi；限制容器使用CPU的大小为0.5到2，内存大小为20Mi-1Gi。其中默认分配250m的CPU（0.25），以及100Mi的内存配额。

使用命令创建资源限制对象：

```shell
kubectl create -f frontend-limit.yaml --namespace=frontend
```

##### **多租户配额管理**

在PaaS章节中，我们遇到过基于Kubernetes构建的PaaS平台，在那种状态下的Kubernetes通常拥有为数不少的用户，以及各类用户组。然而一个集群中的资源是有限的，为了更好的控制分配这些有限的资源，就需要在Namespace上配置ResourceQuota达到用户级别的资源配额。

例如下面的例子，先创建一个新的namespace：

```shell
$ kubectl create namespace myspace
```

然后新建一个定义配额的配置文件：

```shell
$ cat <<EOF > compute-resources.yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: compute-resources
spec:
  hard:
    pods: "4"
    requests.cpu: "1"
    requests.memory: 1Gi
    limits.cpu: "2"
    limits.memory: 2Gi
EOF
```

应用这个配置：

```shell
$ kubectl create -f ./compute-resources.yaml --namespace=myspace
```

现在myspace这个namespace被限制了最多只能运行4个Pod，并且CPU使用最大为2，内存最大为2Gi。

除了限制计算资源，还可以限制资源对象，例如下面配置：

```shell
$ cat <<EOF > object-counts.yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: object-counts
spec:
  hard:
    configmaps: "10"
    persistentvolumeclaims: "4"
    replicationcontrollers: "20"
    secrets: "10"
    services: "10"
    services.loadbalancers: "2"
EOF
```

应用配置到myspace命名空间：

```shell
$ kubectl create -f ./object-counts.yaml --namespace=myspace
```

上面配置限制了cm对象最大值 为10，PVC最大值为4，RC最大值为20，secret、和service最大值为10，最多创建两个负载均衡器。

使用命令查看配额情况：

```shell
$ kubectl get quota --namespace=myspace
NAME                    AGE
compute-resources       30s
object-counts           32s

$ kubectl describe quota compute-resources --namespace=myspace
Name:                  compute-resources
Namespace:             myspace
Resource               Used Hard
--------               ---- ----
limits.cpu             0    2
limits.memory          0    2Gi
pods                   0    4
requests.cpu           0    1
requests.memory        0    1Gi

$ kubectl describe quota object-counts --namespace=myspace
Name:                   object-counts
Namespace:              myspace
Resource                Used    Hard
--------                ----    ----
configmaps              0       10
persistentvolumeclaims  0       4
replicationcontrollers  0       20
secrets                 1       10
services                0       10
services.loadbalancers  0       2
```

### 12.3.2 kubelet垃圾回收机制

Kubernetes系统在长时间运行后，Kubernetes各个节点会下载非常多的镜像，其中存在很多不必要的镜像（过期、旧版、无效的镜像）。同时因为长时间运行大量容器，容器退出后没有及时删除，数据仍旧留在宿主机上。不必要的镜像和已退出的容器都会占用大量的磁盘空间。为此，kubelet会进行垃圾清理工作，即定期清理过期镜像和死亡容器。

不推荐使用其它管理工具或手工进行容器和镜像的清理，因为kubelet需要通过容器来判断pod的运行状态，如果使用其它方式清除容器有可能影响kubelet的正常工作。

#### 1. 容器的GC设置

kubelet定时执行容器清理，每次根据下表12-3中的3个参数选择容器进行删除，通常情况下优先删除创建时间最久的已退出容器，kubelet不会删除非Kubernetes管理的容器。

| 参数                 | kubelet启动参数                             | 说明                      |
| ------------------ | --------------------------------------- | ----------------------- |
| MinAge             | --minimum-container-ttl-duration        | 已退出容器能够被删除的最小TTL，默认是1分钟 |
| MaxPerPodContainer | --maximum-dead-containers-per-container | 每个Pod允许存在的最大退出容器数目，默认是2 |
| MaxContainers      | --maximum-dead-containers               | 允许存在的最大退出容器数目，默认是100    |

表12-3 容器清理参数

#### 2. 镜像的GC设置

镜像清理的策略是：当硬盘空间使用率超过一定的值就开始执行，kubelet执行清理的时候优先清理最久没被使用的镜像。磁盘空间使用率的阈值通过kubelet的启动参数 `--image-gc-high-threshold` 和 `--image-gc-low-threshold` 指定。

## 12.4 集群监控

### 12.4.1 Heapster

Heapster是Kubernetes集群的容器集群监控和性能分析工具，它可以很容易扩展到其他集群管理解决方案。在Kubernetes世界中，cAdvisor agent负责在集群节点运行，收集节点以及容器的监控数据。Heapster则是一个汇总角色，将每个Node上的cAdvisor的数据进行汇总，然后导到第三方工具（如InfluxDB）。

所有获取到的数据都被推到Heapster配置的后端存储中，并还支持数据的可视化。例如流行的InfluxDB加Grafana方案。

下面是在集群Master中快速安装Heapster与InfluxDB：

```shell
git clone https://github.com/kubernetes/heapster
cd heapster
kubectl create -f deploy/kube-config/influxdb/
```

#### 1. 安装Dashboard

如果你不需要太强大的监控与日志采集功能，只是希望有一个美观的界面查看集群状态，可以使用Kubernetes官方的UI界面，只需要一句话即可部署：

```shell
kubectl create -f https://git.io/kube-dashboard
```

稍等片刻，下载一个不到40MB的镜像即可。

使用--namespace可以查看运行状态：

```shell
kubectl get all --namespace kube-system
```

如果你还希望能够把Kubernetes集群运行日志和监控信息汇总记录，就需要其他工具辅助了。下面我们结合InfluxDB与Grafana演示如何搭建一个Kubernetes监控平台。

#### 2. InfluxDB与Grafana

启动Kubernetes群集，确保能够通过kubectl与集群进行交互。接下来部署三个套件：

```shell
$ git clone https://github.com/kubernetes/heapster
$ cd heapster
$ kubectl create -f deploy/kube-config/influxdb/
$ kubectl create -f deploy/kube-config/rbac/heapster-rbac.yaml
```

Grafana服务默认会请求一个LoadBalancer，但如果集群中LoadBalancer不可用，需要手动将其更改为NodePort。然后使用分配给Grafana服务的外部IP访问Grafana（或NodePort访问）。

Grafana默认用户名和密码为admin。登录Grafana后，添加一个InfluxDB的数据源。操作方法和Docker生态章节中一样，InfluxDB的URL是`http://INFLUXDB_HOST:INFLUXDB_PORT` 。数据库名称是`k8s`。InfluxDB默认用户名和密码为root。

Grafana Web界面也可以通过api-server代理访问。一旦创建了上述资源，该URL将在`kubectl cluster-info`中可见。

### 12.4.2 Filebeat

Kubernetes的EFK插件默认使用Fluentd（以DaemonSet的方式启动）来收集日志，并将收集的日志发送给Elasticsearch。但是这种方案有局限性，例如EFK所有日志都在前台输出，且只有一个日志输出文件。

Filebeat作为ELK协议栈的新成员，一个轻量级开源日志文件数据搜集器，基于Logstash-Forwarder源代码开发，目标是替代Logstash。在需要采集日志数据的节点上安装Filebeat，并指定日志目录或日志文件后，Filebeat就能读取数据，迅速发送到Logstash进行解析，亦或直接发送到Elasticsearch进行集中式存储和分析。

创建一个配置文件：

```yaml
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: filebeat-log
  namespace: default
spec:
  replicas: 3
  template:
    metadata:
      labels:
        k8s-log: filebeat-log
    spec:
      containers:
      - image: filebeat:5.4.0
        name: filebeat
        volumeMounts:
        - name: log-vol
          mountPath: /log
        - name: filebeat-config
          mountPath: /etc/filebeat/
      - image: nginx:alpine
        name : nginx
        ports:
        - containerPort: 80
        volumeMounts:
        - name: log-vol
          mountPath: /usr/local/nginx/logs
      volumes:
      - name: log-vol
        emptyDir: {}
      - name: filebeat-config
        configMap:
          name: filebeat-config
---
apiVersion: v1
kind: Service
metadata:
  name: filebeat-log
  labels:
    app: filebeat-log
spec:
  ports:
  - port: 80
    protocol: TCP
    name: http
  selector:
    run: filebeat-log
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: filebeat-config
data:
  filebeat.yml: |
    filebeat.prospectors:
    - input_type: log
      paths:
        - "/log/*"
        - "/log/usermange/common/*"
    output.elasticsearch:
      hosts: ["172.16.168.200:9200"]
    username: "demo"
    password: "demopass"
    index: "filebeat-log"
```

其中`172.16.168.200:9200`是Elasticsearch的服务地址。上面配置中已经包含了Filebeat的ConfigMap。部署这个配置文件：

```shell
kubectl create -f filebeat-log.yaml
```

如果你已经部署了ELK集群，那么现在查看Kibana面板应该就能收到Nginx的访问日志了。

## 12.5 本章总结

本章总结了前面两章关于Kubernetes的各种知识点，并实际操作演示。全书到本章就完全结束了，但是我们对于容器云的探索还远未结束。Kubernetes在面对超大规模的集群部署场景时还是显得力不从心，如果要应用于大规模集群甚至跨域集群中，Kubernetes还需要用户自行改造或者选择其他方案与Kubernetes配合。